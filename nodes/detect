#!/usr/bin/env python3

from mpl_toolkits.mplot3d import Axes3D, axes3d
import matplotlib.pyplot as plt
import io
from ikpy import chain
from hri_fullbody.jointstate import compute_jointstate, \
    HUMAN_JOINT_NAMES, compute_jointstate
from hri_fullbody.rs_to_depth import rgb_to_xyz  # SITW
from hri_fullbody.urdf_generator import make_urdf_human
from protobuf_to_dict import protobuf_to_dict
import math
import uuid
import numpy as np
import time
import sys
import copy
from collections import deque
import subprocess

import rospy
import rosparam
import tf
from message_filters import ApproximateTimeSynchronizer, Subscriber
from sensor_msgs.msg import Image, \
    CompressedImage, CameraInfo, RegionOfInterest
from sensor_msgs.msg import JointState
from hri_msgs.msg import Skeleton2D, PointOfInterest2D, \
    IdsList, RegionOfInterestStamped
from geometry_msgs.msg import Point, PointStamped

from cv_bridge import CvBridge
import cv2

import mediapipe as mp
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_holistic = mp.solutions.holistic
mp_pose = mp.solutions.pose


TEXTUAL_DEBUG = False

# if set to true, face IDs will be generated as a sequence of integers,
# starting at 00001.
# Otherwise, face IDs will be a random set of 5 characters in [0-9a-f]
DETERMINISTIC_ID = False

PREALLOCATE_PUBLISHERS = DETERMINISTIC_ID
PREALLOCATION_SIZE = 150

# nb of pixels between the centers of to successive regions of interest to
# consider they belong to the same person
MAX_ROIS_DISTANCE = 100

# max scale factor between two successive regions of interest to consider they
# belong to the same person
MAX_SCALING_ROIS = 1.2

# Parameter definition for the bounding box resizer
BB_MULT = 0.0
FACE_BB_MULT = 0.5

USE_DEPTH = False
STICKMAN_DEBUG = False

# Mediapipe 2D keypoint order:
# ['nose', 'left_eye_inner', 'left_eye', 'left_eye_outer',
#   'right_eye_inner', 'right_eye', 'right_eye_outer',
#   'left_ear', 'right_ear', 'mouth_left', 'mouth_right',
#   'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
#   'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky',
#   'left_index', 'right_index', 'left_thumb', 'right_thumb',
#   'left_hip', 'right_hip', 'left_knee', 'right_knee',
#   'left_ankle', 'right_ankle', 'left_heel', 'right_heel',
#   'left_foot_index', 'right_foot_index']

ros4hri_to_mediapipe = [None] * 18

ros4hri_to_mediapipe[Skeleton2D.NOSE] = 0
ros4hri_to_mediapipe[Skeleton2D.LEFT_SHOULDER] = 11
ros4hri_to_mediapipe[Skeleton2D.LEFT_ELBOW] = 13
ros4hri_to_mediapipe[Skeleton2D.LEFT_WRIST] = 15
ros4hri_to_mediapipe[Skeleton2D.RIGHT_SHOULDER] = 12
ros4hri_to_mediapipe[Skeleton2D.RIGHT_ELBOW] = 14
ros4hri_to_mediapipe[Skeleton2D.RIGHT_WRIST] = 16
ros4hri_to_mediapipe[Skeleton2D.LEFT_HIP] = 23
ros4hri_to_mediapipe[Skeleton2D.LEFT_KNEE] = 25
ros4hri_to_mediapipe[Skeleton2D.LEFT_ANKLE] = 27
ros4hri_to_mediapipe[Skeleton2D.RIGHT_HIP] = 24
ros4hri_to_mediapipe[Skeleton2D.RIGHT_KNEE] = 26
ros4hri_to_mediapipe[Skeleton2D.RIGHT_ANKLE] = 28
ros4hri_to_mediapipe[Skeleton2D.RIGHT_EYE] = 5
ros4hri_to_mediapipe[Skeleton2D.LEFT_EYE] = 2
ros4hri_to_mediapipe[Skeleton2D.LEFT_EAR] = 7
ros4hri_to_mediapipe[Skeleton2D.RIGHT_EAR] = 8


def _normalized_to_pixel_coordinates(
        normalized_x: float, normalized_y: float, image_width: int,
        image_height: int):

    x_px = min(math.floor(normalized_x * image_width), image_width - 1)
    y_px = min(math.floor(normalized_y * image_height), image_height - 1)
    return x_px, y_px


def _make_2d_skeleton_msg(header, pose_2d):
    skel = Skeleton2D()
    skel.header = header

    # Mediapipe 2D keypoint order:
    # ['nose', 'left_eye_inner', 'left_eye', 'left_eye_outer',
    #   'right_eye_inner', 'right_eye', 'right_eye_outer',
    #   'left_ear', 'right_ear', 'mouth_left', 'mouth_right',
    #   'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
    #   'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky',
    #   'left_index', 'right_index', 'left_thumb', 'right_thumb',
    #   'left_hip', 'right_hip', 'left_knee', 'right_knee',
    #   'left_ankle', 'right_ankle', 'left_heel', 'right_heel',
    #   'left_foot_index', 'right_foot_index']

    skel.skeleton = [None] * 18

    for idx, human_joint in enumerate(ros4hri_to_mediapipe):
        if human_joint is not None:
            skel.skeleton[idx] = PointOfInterest2D(
                x=pose_2d[human_joint].get('x'),
                y=pose_2d[human_joint].get('y'),
                c=pose_2d[human_joint].get('visibility'))

    # There is no Neck landmark in Mediapipe pose estimation
    # However, we can think of the neck point as the average
    # point between left and right shoulder.
    skel.skeleton[Skeleton2D.NECK] = PointOfInterest2D(
        (
            skel.skeleton[Skeleton2D.LEFT_SHOULDER].x
            + skel.skeleton[Skeleton2D.RIGHT_SHOULDER].x
        )
        / 2,
        (
            skel.skeleton[Skeleton2D.LEFT_SHOULDER].y
            + skel.skeleton[Skeleton2D.RIGHT_SHOULDER].y
        )
        / 2,
        min(skel.skeleton[Skeleton2D.LEFT_SHOULDER].c,
            skel.skeleton[Skeleton2D.RIGHT_SHOULDER].c))

    return skel


def _get_bounding_box_limits(face_landmarks, image_width, image_height):
    x_max = 0.0
    y_max = 0.0
    x_min = 1.0
    y_min = 1.0
    # for result in results:
    for data_point in face_landmarks:
        if x_max < data_point.x:
            x_max = data_point.x
        if y_max < data_point.y:
            y_max = data_point.y
        if x_min > data_point.x:
            x_min = data_point.x
        if y_min > data_point.y:
            y_min = data_point.y

    delta_x = x_max - x_min
    delta_y = y_max - y_min

    x_min -= BB_MULT*delta_x
    y_min -= BB_MULT*delta_y
    x_max += BB_MULT*delta_x
    y_max += BB_MULT*delta_y
    x_min, y_min = _normalized_to_pixel_coordinates(
        x_min, y_min, image_width, image_height)
    x_max, y_max = _normalized_to_pixel_coordinates(
        x_max, y_max, image_width, image_height)
    return x_min, y_min, x_max, y_max


def _distance_rois(bb1, bb2):
    x1, y1 = bb1.x_offset + bb1.width/2, bb1.y_offset + bb1.height/2
    x2, y2 = bb2.x_offset + bb2.width/2, bb2.y_offset + bb2.height/2

    return (x1-x2) * (x1-x2) + (y1-y2) * (y1-y2)


class FullbodyDetector:

    def __init__(self, use_depth, visual_debug):

        self.use_depth = use_depth
        self.visual_debug = visual_debug

        self.detector = mp_holistic.Holistic(
            min_detection_confidence=0.5, min_tracking_confidence=0.5)

        self.from_depth_image = False

        self.x_min_face = 1.00
        self.y_min_face = 1.00
        self.x_max_face = 0.00
        self.y_max_face = 0.00

        self.x_min_body = 1.00
        self.y_min_body = 1.00
        self.x_max_body = 0.00
        self.y_max_body = 0.00

        # ID -> (publisher, RoI, nb_frames_visible)
        self.detectedFaces = {}
        self.detectedBodies = {}

        self.visual_pub = rospy.Publisher(
            '/fullbody/visual', Image, queue_size=1)
        self.skel_pub = rospy.Publisher(
            '/fullbody/skeleton', Skeleton2D, queue_size=1)
        self.js_pub = rospy.Publisher(
            '/test_joint_states', JointState, queue_size=1)

        self.br = CvBridge()

        self.last_face_id = 1
        self.last_body_id = 1

        # 1. generate a URDF model for this body, and set it on the
        #    ROS parameter server
        self.body_id = "test"
        # test for just one person detected
        self.urdf = make_urdf_human(self.body_id)
        rospy.loginfo("Setting URDF description for body"
                      "<%s> (param name: human_description_%s)" % (
                          self.body_id, self.body_id))
        self.human_description = "human_description_%s" % self.body_id
        rosparam.set_param_raw(self.human_description, self.urdf)

        self.urdf_file = io.StringIO(self.urdf)
        self.r_arm_chain = chain.Chain.from_urdf_file(
            self.urdf_file,
            base_elements=[
                "r_y_shoulder_%s" % self.body_id],
            base_element_type="joint",
            active_links_mask=[False, True, True, True, True, False])
        self.urdf_file.seek(0)
        self.l_arm_chain = chain.Chain.from_urdf_file(
            self.urdf_file,
            base_elements=[
                "l_y_shoulder_%s" % self.body_id],
            base_element_type="joint",
            active_links_mask=[False, True, True, True, True, False])
        self.urdf_file.seek(0)
        self.r_leg_chain = chain.Chain.from_urdf_file(
            self.urdf_file,
            base_elements=[
                "r_y_hip_%s" % self.body_id],
            base_element_type="joint",
            active_links_mask=[False, True, True, True, False])
        self.urdf_file.seek(0)
        self.l_leg_chain = chain.Chain.from_urdf_file(
            self.urdf_file,
            base_elements=[
                "l_y_hip_%s" % self.body_id],
            base_element_type="joint",
            active_links_mask=[False, True, True, True, False])

        self.ik_chains = {}  # maps a body id to the IKpy chains
        self.ik_chains[self.body_id] = [
            self.r_arm_chain,
            self.l_arm_chain,
            self.r_leg_chain,
            self.l_leg_chain
        ]

        # 2. create the publishers for the 2D skeleton and joint state
        #prefix = TOPIC_PREFIX + "%s/" % self.body_id
        # self.publishers[body_id] = [
        #rospy.Publisher(prefix + "skeleton2d", Skeleton2D, queue_size=1),
        #rospy.Publisher(prefix + "joint_states", JointState, queue_size=1)
        # ] ==> At this point, as I am only testing the human model,
        # I am using my own defined publishers

        # 3. spawn a new robot_state_publisher process, which will publish the
        #    TF frames for this body
        rospy.loginfo(
            "Spawning a instance of robot_state_publisher for this body...")
        cmd = ["rosrun", "robot_state_publisher", "robot_state_publisher",
               "__name:=robot_state_publisher_body_%s" % self.body_id,
               "joint_states:=%s" % ("test_joint_states"),
               "robot_description:=%s" % self.human_description,
               "_publish_frequency:=25",
               "_use_tf_static:=false"
               ]
        rospy.loginfo("Executing: %s" % " ".join(cmd))
        proc = subprocess.Popen(cmd,
                                stdout=sys.stdout,
                                stderr=subprocess.STDOUT)
        self.robot_state_publishers = {}
        self.robot_state_publishers[self.body_id] = proc

        if PREALLOCATE_PUBLISHERS:
            self.face_pubs = {"%05d" % i: rospy.Publisher(
                "/humans/faces/%05d/roi" % i,
                RegionOfInterestStamped,
                queue_size=1)
                for i in range(1, PREALLOCATION_SIZE)}
            self.body_pubs = {"%05d" % i: rospy.Publisher(
                "/humans/bodies/%05d/roi" % i,
                RegionOfInterestStamped,
                queue_size=1)
                for i in range(1, PREALLOCATION_SIZE)}

        self.faces_pub = rospy.Publisher(
            "/humans/faces/tracked", IdsList, queue_size=1)
        self.bodies_pub = rospy.Publisher(
            "/humans/bodies/tracked", IdsList, queue_size=1)

        self.tb = tf.TransformBroadcaster()

    def make_jointstate(
            self,
            body_id,
            original_image,
            pose_3d,
            pose_2d,
            header):

        js = JointState()
        js.header = copy.copy(original_image.header)
        js.name = [jn + "_%s" % body_id for jn in HUMAN_JOINT_NAMES]

        if self.from_depth_image:
            torso_px = _normalized_to_pixel_coordinates(
                (
                    pose_3d[23].get('x')
                    + pose_3d[24].get('x')
                )
                / 2,
                (
                    pose_3d[23].get('y')
                    + pose_3d[24].get('y')
                )
                / 2,
                self.img_width,
                self.img_height)
            torso = self.ask_for_position(torso_px[0], torso_px[1])
            l_wrist_px = _normalized_to_pixel_coordinates(pose_3d[15].get(
                'x'), pose_3d[15].get('y'), self.img_width, self.img_height)
            l_wrist = self.ask_for_position(l_wrist_px[0], l_wrist_px[1])
            l_ankle_px = _normalized_to_pixel_coordinates(pose_3d[27].get(
                'x'), pose_3d[27].get('y'), self.img_width, self.img_height)
            l_ankle = self.ask_for_position(l_ankle_px[0], l_ankle_px[1])
            r_wrist_px = _normalized_to_pixel_coordinates(pose_3d[16].get(
                'x'), pose_3d[16].get('y'), self.img_width, self.img_height)
            r_wrist = self.ask_for_position(r_wrist_px[0], r_wrist_px[1])
            r_ankle_px = _normalized_to_pixel_coordinates(pose_3d[28].get(
                'x'), pose_3d[28].get('y'), self.img_width, self.img_height)
            r_ankle = self.ask_for_position(r_ankle_px[0], r_ankle_px[1])
            nose_px = _normalized_to_pixel_coordinates(pose_3d[0].get(
                'x'), pose_3d[0].get('y'), self.img_width, self.img_height)
            nose = self.ask_for_position(nose_px[0], nose_px[1])
            feet_px = _normalized_to_pixel_coordinates(
                (
                    pose_3d[31].get('x')
                    + pose_3d[32].get('x')
                )
                / 2,
                (pose_3d[31].get('y')
                 + pose_3d[32].get('y')
                 )
                / 2,
                self.img_width,
                self.img_height)
            feet = self.ask_for_position(feet_px[0], feet_px[1])
        else:
            torso = np.array([
                -(
                    pose_3d[23].get('z')
                    + pose_3d[24].get('z')
                )
                / 2,
                (
                    pose_3d[23].get('x')
                    + pose_3d[24].get('x')
                )
                / 2,
                -(
                    pose_3d[23].get('y')
                    + pose_3d[24].get('y')
                )
                / 2
            ])
            l_shoulder = np.array([
                -pose_3d[11].get('z'),
                pose_3d[11].get('x'),
                -pose_3d[11].get('y')-0.605
            ])
            l_elbow = np.array([
                -pose_3d[13].get('z'),
                pose_3d[13].get('x'),
                -pose_3d[13].get('y')-0.605
            ])
            l_wrist = np.array([
                -pose_3d[15].get('z'),
                pose_3d[15].get('x'),
                -pose_3d[15].get('y')-0.605
            ])
            l_ankle = np.array([
                -pose_3d[27].get('z'),
                pose_3d[27].get('x'),
                -pose_3d[27].get('y')
            ])
            r_shoulder = np.array([
                -pose_3d[12].get('z'),
                pose_3d[12].get('x'),
                -pose_3d[12].get('y')-0.605
            ])
            r_elbow = np.array([
                -pose_3d[14].get('z'),
                pose_3d[14].get('x'),
                -pose_3d[14].get('y')-0.605
            ])
            r_wrist = np.array([
                -pose_3d[16].get('z'),
                pose_3d[16].get('x'),
                -pose_3d[16].get('y')-0.605
            ])
            r_ankle = np.array([
                -pose_3d[28].get('z'),
                pose_3d[28].get('x'),
                -pose_3d[28].get('y')
            ])
            nose = np.array([
                -pose_3d[0].get('z'),
                pose_3d[0].get('x'),
                -pose_3d[0].get('y')
            ])
            feet = np.array([
                -(
                    pose_3d[32].get('z')
                    + pose_3d[31].get('z')
                )
                / 2,
                (
                    pose_3d[32].get('x')
                    + pose_3d[31].get('x')
                )
                / 2,
                -(
                    pose_3d[32].get('y')
                    + pose_3d[31].get('y')
                )
                / 2
            ])

        ### depth and rotation ###

        torso_px = _normalized_to_pixel_coordinates(
            (pose_2d[23].get('x')+pose_2d[24].get('x'))/2,
            (pose_2d[23].get('y')+pose_2d[24].get('y'))/2,
            self.img_width,
            self.img_height)
        theta = np.arctan2(pose_3d[24].get('x'), -pose_3d[24].get('z'))
        if self.use_depth:
            torso_res = rgb_to_xyz(
                torso_px[0],
                torso_px[1],
                self.rgb_info,
                self.depth_info,
                self.image_depth
            )
        else:
            torso_res = np.array([0, 0, 0])

        ### Publishing tf transformations ###
        self.tb.sendTransform(
            (torso_res[2], -torso_res[0], 0.0),
            tf.transformations.quaternion_from_euler(
                0,
                0,
                1.5*np.pi+theta
            ),
            header.stamp,
            "body_test",
            "camera_link"
        )

        if STICKMAN_DEBUG:
            self.tb.sendTransform(
                (torso[0]+torso_res[2], torso[1]-torso_res[0], torso[2]),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    1.5*np.pi+theta
                ),
                header.stamp,
                "mediapipe_torso",
                "camera_link"
            )
            self.tb.sendTransform(
                (0.0, 0.0, 0.605),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "our_torso",
                "mediapipe_torso"
            )
            self.tb.sendTransform(
                (l_shoulder[0], l_shoulder[1], l_shoulder[2]),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "left_shoulder",
                "our_torso"
            )
            self.tb.sendTransform(
                (r_shoulder[0], r_shoulder[1], r_shoulder[2]),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "right_shoulder",
                "our_torso"
            )
            self.tb.sendTransform(
                (l_elbow[0]-l_shoulder[0],
                 l_elbow[1]-l_shoulder[1],
                 l_elbow[2]-l_shoulder[2]
                 ),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "left_elbow",
                "left_shoulder"
            )
            self.tb.sendTransform(
                (r_elbow[0]-r_shoulder[0],
                 r_elbow[1]-r_shoulder[1],
                 r_elbow[2]-r_shoulder[2]
                 ),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "right_elbow",
                "right_shoulder"
            )
            self.tb.sendTransform(
                (l_wrist[0]-l_elbow[0],
                 l_wrist[1]-l_elbow[1],
                 l_wrist[2]-l_elbow[2]),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "left_wrist",
                "left_elbow"
            )
            self.tb.sendTransform(
                (r_wrist[0]-r_elbow[0],
                 r_wrist[1]-r_elbow[1],
                 r_wrist[2]-r_elbow[2]
                 ),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "right_wrist",
                "right_elbow"
            )
            self.tb.sendTransform(
                (l_ankle[0],
                 l_ankle[1],
                 l_ankle[2]
                 ),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0),
                header.stamp,
                "left_ankle",
                "mediapipe_torso"
            )
            self.tb.sendTransform(
                (r_ankle[0],
                 r_ankle[1],
                 r_ankle[2]
                 ),
                tf.transformations.quaternion_from_euler(
                    0,
                    0,
                    0
                ),
                header.stamp,
                "right_ankle",
                "mediapipe_torso"
            )
        js.position = compute_jointstate(
            self.ik_chains[body_id], torso,
            l_wrist,
            l_ankle,
            r_wrist,
            r_ankle
        )

        if self.use_depth:
            nose_px = [pose_2d[0].get('x'), pose_2d[0].get('y')]
            nose_px = _normalized_to_pixel_coordinates(
                nose_px[0],
                nose_px[1],
                self.img_width,
                self.img_height)
            left_foot_px = [
                pose_2d[31].get('x'),
                pose_2d[31].get('y')]
            left_foot_px = _normalized_to_pixel_coordinates(
                left_foot_px[0],
                left_foot_px[1],
                self.img_width,
                self.img_height)
            nose_3d = rgb_to_xyz(
                nose_px[0],
                nose_px[1],
                self.rgb_info,
                self.depth_info,
                self.image_depth)
            left_foot_3d = rgb_to_xyz(
                left_foot_px[0],
                left_foot_px[1],
                self.rgb_info,
                self.depth_info,
                self.image_depth)

            if pose_2d[0].get('visibility') > 0.85 and \
                    pose_2d[31].get('visibility') > 0.85:
                #print('3d estimated height: ', left_foot_3d[1] - nose_3d[1])
                estimated_height = left_foot_3d[1] - nose_3d[1]

        return js

    def find_previous_match(
            self,
            bb,
            dict,
            max_rois_distance,
            max_scaling_rois):

        for id, value in dict.items():
            _, prev_bb, _ = value
            rois_distance = _distance_rois(prev_bb, bb)
            if (rois_distance
                    < max_rois_distance * max_rois_distance
                    and (
                        1/max_scaling_rois
                        < prev_bb.width/bb.width
                                < max_scaling_rois
                        )
                    and (
                        1/max_scaling_rois
                        < prev_bb.height/bb.height
                                < max_scaling_rois
                        )
                    ):
                return id

        return None

    def generate_face_id(self):

        if DETERMINISTIC_ID:
            id_str = "%05d" % self.last_face_id
            self.last_face_id = (self.last_face_id + 1) % 10000
            return id_str

        else:
            return str(uuid.uuid4())[:5]  # for a 5 char long ID

    def generate_body_id(self):

        if DETERMINISTIC_ID:
            id_str = "%05d" % self.last_body_id
            self.last_body_id = (self.last_body_id + 1) % 10000
            return id_str

        else:
            return str(uuid.uuid4())[:5]  # for a 5 char long ID

    def generate_tmp_id(self):
        return str(uuid.uuid4())[:5]  # for a 5 char long ID

    # The body bounding box needs correction
    # for the way it is initially computed.
    def correct_body_bounding_box(self):

        delta_face_y = self.y_max_face - self.y_min_face
        delta_face_x = self.x_max_face - self.x_min_face
        delta_face = max(delta_face_x, delta_face_y)
        corrected_min_x = int(self.x_min_body-(FACE_BB_MULT*delta_face))
        corrected_min_y = int(self.y_min_body-(FACE_BB_MULT*delta_face))
        corrected_max_x = int(self.x_max_body+(FACE_BB_MULT*delta_face))
        corrected_max_y = int(self.y_max_body+(FACE_BB_MULT*delta_face))

        return corrected_min_x, corrected_min_y, corrected_max_x, corrected_max_y

    def detect(self, image_rgb, header):

        img_height, img_width, _ = image_rgb.shape
        self.img_height, self.img_width = img_height, img_width

        image_rgb.flags.writeable = False
        image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)
        results = self.detector.process(image_rgb)
        image_rgb.flags.writeable = True

        currentFaces = {}  # This structure here is specifically thought
        # for several faces detection within the same image,
        #which is something
        # we can not do with mediapipe holistic model estimation.
        currentBodies = {}  # Same concept

        # ATTENTION: code here is partially developed for just one person,
        # partially for multi-person detection.
        # This should be fixed in the next versions ###

        ######## Face Detection Process ########

        if hasattr(results.face_landmarks, 'landmark'):
            (self.x_min_face,
             self.y_min_face,
             self.x_max_face,
             self.y_max_face) = _get_bounding_box_limits(
                results.face_landmarks.landmark,
                img_width,
                img_height
            )
            bb = RegionOfInterest(
                self.x_min_face,
                self.y_min_face,
                self.x_max_face - self.x_min_face,
                self.y_max_face-self.y_min_face,
                True
            )
            id = self.find_previous_match(
                    bb, 
                    self.detectedFaces, 
                    MAX_ROIS_DISTANCE, 
                    MAX_SCALING_ROIS)
            if id:
                # we re-detect a face: if it is a 2nd frame,
                # we create a publisher for it.
                if not self.detectedFaces[id][0]:
                    final_id = self.generate_face_id()
                    rospy.loginfo("New face [face_%s]" % final_id)

                    if PREALLOCATE_PUBLISHERS:
                        pub = self.face_pubs[final_id]
                    else:
                        pub = rospy.Publisher(
                            "/humans/faces/%s/roi" % final_id,
                            RegionOfInterestStamped,
                            queue_size=1
                        )

                    currentFaces[final_id] = (
                        pub, bb, self.detectedFaces[id][2] + 1)
                else:
                    currentFaces[id] = (
                        self.detectedFaces[id][0],
                        bb,
                        self.detectedFaces[id][2] + 1)
            else:
                # we 'provisionally' store the face - we'll create a publisher
                # and start publishing only if we see the
                # face a second time
                id = self.generate_tmp_id()
                currentFaces[id] = (None, bb, 1)
                print('New face! Id:', id)
            for id, value in self.detectedFaces.items():
                if id not in currentFaces:
                    pub, _, nb_frames = value
                    if pub:
                        rospy.loginfo(
                            "Face [face_%s] lost."
                            "It remained visible for %s frames"
                            % (id, nb_frames))
                        pub.unregister()

            self.detectedFaces = currentFaces

            list_ids = []

            for id, value in self.detectedFaces.items():
                pub, bb, _ = value
                if pub:
                    list_ids.append(str(id))
                    pub.publish(RegionOfInterestStamped(header, bb))

            self.faces_pub.publish(IdsList(header, list_ids))

            if TEXTUAL_DEBUG:
                print('FACE: bb limits ==> x_min =', self.x_min_face,
                      'y_min = ', self.y_min_face,
                      'x_max = ', self.x_max_face,
                      'y_max = ', self.y_max_face
                      )
            if self.visual_debug:
                image_rgb = cv2.rectangle(
                    image_rgb,
                    (max(self.x_min_face, 0),
                     max(self.y_min_face, 0)),
                    (min(self.x_max_face, img_width-1),
                     min(self.y_max_face, img_height-1)),
                    (0, 0, 255),
                    3
                )

        ########################################

        ######## Introducing Hand Landmarks ########

        if hasattr(results.left_hand_landmarks, 'landmark'):
            pose_keypoints = protobuf_to_dict(results.pose_landmarks)
            pose_kpt = pose_keypoints.get('landmark')
            landmarks = [None] * 21
            for i in range(0, 21):
                landmarks[i] = PointOfInterest2D(
                    pose_kpt[i].get('x'),
                    pose_kpt[i].get('y'),
                    pose_kpt[i].get('visibility')
                )
            (self.x_min_hand_left,
             self.y_min_hand_left,
             self.x_max_hand_left,
             self.y_max_hand_left) = _get_bounding_box_limits(landmarks,
                                                              img_width,
                                                              img_height)

        if hasattr(results.right_hand_landmarks, 'landmark'):
            pose_keypoints = protobuf_to_dict(results.pose_landmarks)
            pose_kpt = pose_keypoints.get('landmark')
            landmarks = [None] * 21
            for i in range(0, 21):
                landmarks[i] = PointOfInterest2D(
                    pose_kpt[i].get('x'),
                    pose_kpt[i].get('y'),
                    pose_kpt[i].get('visibility')
                )
            (self.x_min_hand_right,
             self.y_min_hand_right,
             self.x_max_hand_right,
             self.y_max_hand_right) = _get_bounding_box_limits(landmarks,
                                                               img_width,
                                                               img_height)

        ############################################

        ######## Body Detection Process ########

        if hasattr(results.pose_landmarks, 'landmark'):
            pose_keypoints = protobuf_to_dict(results.pose_landmarks)
            pose_world_keypoints = protobuf_to_dict(
                results.pose_world_landmarks)
            #mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)
            pose_kpt = pose_keypoints.get('landmark')
            pose_world_kpt = pose_world_keypoints.get('landmark')
            skel_msg = _make_2d_skeleton_msg(header, pose_kpt)
            js = self.make_jointstate(
                self.body_id,
                self.image_for_jointstate,
                pose_world_kpt,
                pose_kpt,
                header
            )
            self.js_pub.publish(js)
            self.skel_pub.publish(skel_msg)
            (self.x_min_body,
             self.y_min_body,
             self.x_max_body,
             self.y_max_body) = _get_bounding_box_limits(
                skel_msg.skeleton,
                img_width,
                img_height
            )
            self.skeleton = skel_msg.skeleton
            if (hasattr(results.right_hand_landmarks, 'landmark') and
                hasattr(results.face_landmarks, 'landmark') and
                    hasattr(results.left_hand_landmarks, 'landmark')):
                self.x_min_body = min(
                    self.x_min_body,
                    self.x_min_face,
                    self.x_min_hand_left,
                    self.x_min_hand_right
                )
                self.y_min_body = min(
                    self.y_min_body,
                    self.y_min_face,
                    self.y_min_hand_left,
                    self.y_min_hand_right
                )
                self.x_max_body = max(
                    self.x_max_body,
                    self.x_max_face,
                    self.x_max_hand_left,
                    self.x_max_hand_right
                )
                self.y_max_body = max(
                    self.y_max_body,
                    self.y_max_face,
                    self.y_max_hand_left,
                    self.y_max_hand_right
                )
            elif hasattr(results.face_landmarks, 'landmark'):
                self.x_min_body = min(self.x_min_body, self.x_min_face)
                self.y_min_body = min(self.y_min_body, self.y_min_face)
                self.x_max_body = max(self.x_max_body, self.x_max_face)
                self.y_max_body = max(self.y_max_body, self.y_max_face)

            if hasattr(results.right_hand_landmarks, 'landmark'):
                (self.x_min_body,
                 self.y_min_body,
                 self.x_max_body,
                 self.y_max_body) = self.correct_body_bounding_box()

            bb = RegionOfInterest(
                self.x_min_body,
                self.y_min_body,
                self.x_max_body - self.x_min_body,
                self.y_max_body - self.y_min_body,
                True
            )
            id = self.find_previous_match(
                    bb, 
                    self.detectedBodies,
                    MAX_ROIS_DISTANCE*2,
                    MAX_SCALING_ROIS*2)
            if id:
                # we re-detect a face: if it is a 2nd frame,
                # we create a publisher for it.
                if not self.detectedBodies[id][0]:
                    final_id = self.generate_body_id()
                    rospy.loginfo("New body [body_%s]" % final_id)

                    if PREALLOCATE_PUBLISHERS:
                        pub = self.body_pubs[final_id]
                    else:
                        pub = rospy.Publisher(
                            "/humans/bodies/%s/roi" % final_id,
                            RegionOfInterestStamped,
                            queue_size=1
                        )

                    currentBodies[final_id] = (
                        pub, bb, self.detectedBodies[id][2] + 1)  # LEFT HERE!
                else:
                    currentBodies[id] = (
                        self.detectedBodies[id][0],
                        bb,
                        self.detectedBodies[id][2] + 1
                    )
            else:
                # we 'provisionally' store the face - we'll create a
                # publisher and start publishing only if we see the
                # face a second time
                id = self.generate_tmp_id()
                currentBodies[id] = (None, bb, 1)
                print('New body! Id:', id)
            for id, value in self.detectedBodies.items():
                if id not in currentBodies:
                    pub, _, nb_frames = value
                    if pub:
                        rospy.loginfo(
                            "Body [body_%s] lost. "
                            "It remained visible for %s frames"
                            % (id, nb_frames))
                        pub.unregister()

            self.detectedBodies = currentBodies

            list_ids = []

            for id, value in self.detectedBodies.items():
                pub, bb, _ = value
                if pub:
                    list_ids.append(str(id))
                    pub.publish(RegionOfInterestStamped(header, bb))

            self.bodies_pub.publish(IdsList(header, list_ids))

            if TEXTUAL_DEBUG:
                print(
                    'BODY: bb limits ==> x_min =', self.x_min_body,
                    'y_min = ', self.y_min_body,
                    'x_max = ', self.x_max_body,
                    'y_max = ', self.y_max_body
                )
            if self.visual_debug:
                image_rgb = cv2.rectangle(
                    image_rgb,
                    (max(self.x_min_body, 0),
                     max(self.y_min_body, 0)),
                    (min(self.x_max_body, img_width-1),
                     min(self.y_max_body, img_height-1)),
                    (255, 0, 0),
                    3
                )

        ########################################

        if self.visual_debug:
            self.visual_pub.publish(
                CvBridge.cv2_to_imgmsg(self.br, image_rgb, "rgb8"))

    def image_callback(self, depth_info, rgb_info, depth_img, rgb_img):

        br = CvBridge()
        image_rgb = br.compressed_imgmsg_to_cv2(rgb_img)
        # Realsense depth encoding: 16UC1
        image_depth = self.br.imgmsg_to_cv2(depth_img, "16UC1")
        self.image_depth = image_depth
        self.image_for_jointstate = rgb_img
        if depth_img.header.stamp > rgb_img.header.stamp:
            header = copy.copy(depth_img.header)
        else:
            header = copy.copy(rgb_img.header)
        self.depth_info = depth_info
        self.rgb_info = rgb_info
        self.detect(image_rgb, header)

    def image_callback_rgb_only(self, rgb_info, rgb_img):

        br = CvBridge()
        image_rgb = br.compressed_imgmsg_to_cv2(rgb_img)
        self.image_for_jointstate = rgb_img
        header = copy.copy(rgb_img.header)
        self.rgb_info = rgb_info
        self.detect(image_rgb, header)

    def init_node(self):

        if self.use_depth:
            self.tss = ApproximateTimeSynchronizer(
                [
                    Subscriber(
                        "/depth_info",
                        CameraInfo,
                        queue_size=1,
                        buff_size=2*24),
                    Subscriber(
                        "/rgb_info",
                        CameraInfo,
                        queue_size=1,
                        buff_size=2**24),
                    Subscriber(
                        "/depth_image",
                        Image,
                        queue_size=1,
                        buff_size=2**24),
                    Subscriber(
                        "/rgb_image",
                        CompressedImage,
                        queue_size=1,
                        buff_size=2**24)
                ],
                1,
                0.05
            )
            self.tss.registerCallback(self.image_callback)
        else:
            self.tss = ApproximateTimeSynchronizer(
                [
                    Subscriber(
                        "/rgb_info",
                        CameraInfo,
                        queue_size=1,
                        buff_size=2**24),
                    Subscriber(
                        "/rgb_image",
                        CompressedImage,
                        queue_size=1,
                        buff_size=2**24)
                ],
                1,
                0.05
            )
            self.tss.registerCallback(self.image_callback_rgb_only)


if __name__ == '__main__':
    rospy.init_node('fullbody_estimation', anonymous=True)
    use_depth = rospy.get_param('~use_depth')
    visual_debug = rospy.get_param('~visual_debug')
    dtct = FullbodyDetector(use_depth, visual_debug)
    dtct.init_node()
    rospy.spin()
    cv2.destroyAllWindows()
